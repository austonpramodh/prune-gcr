#!/usr/bin/env python3

import argparse
import json
import re
import subprocess

from datetime import datetime, timedelta


def main():
    parser = argparse.ArgumentParser(
        description="Prune old Docker images from the Google Container Registry. Example usage:\nprune-gcr www"
    )
    parser.add_argument(
        "repository",
        help="full name of the GCR image repository (as in: gcr.io/project/repository)",
    )
    parser.add_argument(
        "--older-than",
        default=datetime.today().date() - timedelta(90),
        type=date,
        metavar="CUTOFF",
        help="prune images older than CUTOFF. Must be formatted as YYYY-MM-DD. Uses GCP's time zone. Defaults to 90 days ago.",
    )
    parser.add_argument(
        "--keep-at-least",
        default=50,
        type=int,
        metavar="N",
        help="keep at least N images, even if they are older than the cutoff date. Defaults to 50.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="don't actually remove the old images, just print them",
    )
    parser.add_argument(
        "--filter-tags",
        default="",
        type=ascii,
        metavar="TAG",
        help="prune images that match the filter tag TAG",
    )
    args = parser.parse_args()

    repository = args.repository
    print(f"Querying GCP for images in {repository}...")
    digests = get_digests_to_prune(
        repository, args.older_than, keep_at_least=args.keep_at_least, filter_regex=args.filter_tags
    )

    if args.dry_run:
        if len(digests) > 0:
            print(
                "This is a dry run mode. Run without --dry-run to remove the following images:"
            )
        for digest in digests:
            print(f"- {digest}")
    else:
        delete_images(repository, digests)


def date(date_string):
    return datetime.strptime(date_string, "%Y-%m-%d").date()


def get_digests_to_prune(repository, older_than, keep_at_least=0, filter_regex=""):
    entries = get_all_image_entries(repository)

    try:
        retention_index = last_index_older_than(entries, older_than)
    except ValueError as error:
        print(error)
        # All of the entries are older than the specified date so we want to keep all of
        # them
        return []

    retention_count = max(
        len(entries) - retention_index, min(keep_at_least, len(entries))
    )
    retention_index = len(entries) - retention_count

    # filter with the tag on entries
    tag_filtered_entries = get_tag_filtered_image_entries(entries[:retention_index], filter_regex)

    print(f"{len(tag_filtered_entries)} out of {len(entries)} images are eligible to be pruned")
    
    return [entry["digest"] for entry in tag_filtered_entries]


def get_all_image_entries(repository):
    json_array = subprocess.check_output(
        [
            "gcloud",
            "container",
            "images",
            "list-tags",
            repository,
            "--limit",
            "999999",
            "--sort-by",
            "timestamp",
            "--format",
            "json(digest, timestamp, tags)",
            "--quiet",
        ]
    )
    return json.loads(json_array)


def last_index_older_than(entries, older_than):
    # The entries are ordered from oldest to most recent, so iterate through them
    # backwards to find the first one older than the specified date
    for index, entry in reversed(list(enumerate(entries))):
        # Reformat the timestamp so Python can parse it
        timestamp = re.sub(r"[-+](\d\d):(\d\d)$", r"", entry["timestamp"]["datetime"])
        entry_time = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S").date()
        if entry_time < older_than:
            return index + 1
    raise ValueError(
        "There are no entries older than {cutoff}".format(
            cutoff=older_than.strftime("%Y-%m-%d")
        )
    )

def get_tag_filtered_image_entries(entries, filter_regex):
    filtered_entries = []
    for entry in entries:
        tags = entry["tags"]
        # Try matching regex and see if any tag matches
        compiled_filter_regex=re.compile(filter_regex.strip('\''))
        if all(re.match(compiled_filter_regex, tag) for tag in tags):
            filtered_entries.append(entry)
    return filtered_entries


def delete_images(repository, digests):
    if len(digests) == 0:
        print("There are no images to prune; skipping pruning")
        return

    print(f"Deleting {len(digests)} images...")
    for digest_chunk in chunks(digests, 100):
        subprocess.check_output(
            ["gcloud", "container", "images", "delete"]
            + ["{}@{}".format(repository, digest) for digest in digest_chunk]
            + ["--force-delete-tags", "--quiet"]
        )
    print(f"Finished pruning images in {repository}")


def chunks(seq, size):
    return (seq[index : index + size] for index in range(0, len(seq), size))


if __name__ == "__main__":
    main()

# To delete an entire Google Cloud Storage folder for a repository, ONLY after deleting
# all images for a repository, run:
#
# gsutil -m rm -r gs://artifacts.<project>.appspot.com/containers/repositories/library/<repository>
#
# BE VERY CAREFUL!
